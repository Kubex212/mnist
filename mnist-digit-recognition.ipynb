{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mndata = MNIST(\"C:\\\\Users\\\\jkobo\\\\Downloads\\\\mnist\\\\data\")\n",
    "mndata.gz = True\n",
    "train_images, train_labels = mndata.load_training()\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "\n",
    "# Convert to NumPy arrays for easier manipulation\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    num_samples = len(labels)\n",
    "    one_hot_labels = np.zeros((num_samples, num_classes))\n",
    "    for i in range(num_samples):\n",
    "        one_hot_labels[i][labels[i]] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "num_classes = 10  # 0 to 9\n",
    "train_labels = one_hot_encode(train_labels, num_classes)\n",
    "test_labels = one_hot_encode(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "input_size = 784  # 28x28 image dimensions\n",
    "hidden1_size = 128\n",
    "hidden2_size = 64\n",
    "output_size = num_classes\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(0)\n",
    "\n",
    "weights_input_hidden1 = np.random.randn(input_size, hidden1_size)\n",
    "bias_hidden1 = np.zeros(hidden1_size)\n",
    "weights_hidden1_hidden2 = np.random.randn(hidden1_size, hidden2_size)\n",
    "bias_hidden2 = np.zeros(hidden2_size)\n",
    "weights_hidden2_output = np.random.randn(hidden2_size, output_size)\n",
    "bias_output = np.zeros(output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function (sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function (ReLU)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of the ReLU function\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 90807.2841\n",
      "Epoch 2/10, Loss: 0.1000\n",
      "Epoch 3/10, Loss: 0.1000\n",
      "Epoch 4/10, Loss: 0.1000\n",
      "Epoch 5/10, Loss: 0.1000\n",
      "Epoch 6/10, Loss: 0.1000\n",
      "Epoch 7/10, Loss: 0.1000\n",
      "Epoch 8/10, Loss: 0.1000\n",
      "Epoch 9/10, Loss: 0.1000\n",
      "Epoch 10/10, Loss: 0.1000\n"
     ]
    }
   ],
   "source": [
    "# Define the learning rate\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation\n",
    "    hidden1_input = np.dot(train_images, weights_input_hidden1) + bias_hidden1\n",
    "    hidden1_output = relu(hidden1_input)\n",
    "    hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + bias_hidden2\n",
    "    hidden2_output = relu(hidden2_input)\n",
    "    output_input = np.dot(hidden2_output, weights_hidden2_output) + bias_output\n",
    "    output = relu(output_input)\n",
    "\n",
    "    # Calculate the loss (mean squared error)\n",
    "    loss = np.mean((output - train_labels) ** 2)\n",
    "\n",
    "    # Backpropagation\n",
    "    d_output = 2 * (output - train_labels)\n",
    "    d_output *= relu_derivative(output)\n",
    "    d_hidden2 = np.dot(d_output, weights_hidden2_output.T)\n",
    "    d_hidden2 *= relu_derivative(hidden2_output)\n",
    "    d_hidden1 = np.dot(d_hidden2, weights_hidden1_hidden2.T)\n",
    "    d_hidden1 *= relu_derivative(hidden1_output)\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights_hidden2_output -= np.dot(hidden2_output.T, d_output) * learning_rate\n",
    "    bias_output -= np.sum(d_output, axis=0) * learning_rate\n",
    "    weights_hidden1_hidden2 -= np.dot(hidden1_output.T, d_hidden2) * learning_rate\n",
    "    bias_hidden2 -= np.sum(d_hidden2, axis=0) * learning_rate\n",
    "    weights_input_hidden1 -= np.dot(train_images.T, d_hidden1) * learning_rate\n",
    "    bias_hidden1 -= np.sum(d_hidden1, axis=0) * learning_rate\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 9.80%\n"
     ]
    }
   ],
   "source": [
    "# Testing the Neural Network\n",
    "hidden1_input = np.dot(test_images, weights_input_hidden1) + bias_hidden1\n",
    "hidden1_output = relu(hidden1_input)\n",
    "hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + bias_hidden2\n",
    "hidden2_output = relu(hidden2_input)\n",
    "output_input = np.dot(hidden2_output, weights_hidden2_output) + bias_output\n",
    "output = relu(output_input)\n",
    "\n",
    "predicted_labels = np.argmax(output, axis=1)\n",
    "\n",
    "accuracy = np.mean(predicted_labels == np.argmax(test_labels, axis=1))\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "powyższe nie działa - chyba można to wywalić  \n",
    "poniższe działa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1539, Test Accuracy: 92.51%\n",
      "Epoch 2/10, Loss: 0.0720, Test Accuracy: 94.51%\n",
      "Epoch 3/10, Loss: 0.0377, Test Accuracy: 95.85%\n",
      "Epoch 4/10, Loss: 0.0228, Test Accuracy: 96.43%\n",
      "Epoch 5/10, Loss: 0.0157, Test Accuracy: 96.76%\n",
      "Epoch 6/10, Loss: 0.0113, Test Accuracy: 96.99%\n",
      "Epoch 7/10, Loss: 0.0082, Test Accuracy: 97.19%\n",
      "Epoch 8/10, Loss: 0.0060, Test Accuracy: 97.20%\n",
      "Epoch 9/10, Loss: 0.0043, Test Accuracy: 97.27%\n",
      "Epoch 10/10, Loss: 0.0032, Test Accuracy: 97.32%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mndata = MNIST(\"C:\\\\Users\\\\jkobo\\\\Downloads\\\\mnist\\\\data\")  # Replace with the path to your MNIST data\n",
    "mndata.gz = True\n",
    "\n",
    "# Load the training and testing data\n",
    "train_images, train_labels = mndata.load_training()\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "\n",
    "# Convert to NumPy arrays for easier manipulation\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = 10\n",
    "train_labels = np.eye(num_classes)[train_labels]\n",
    "test_labels = np.eye(num_classes)[test_labels]\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 784  # 28x28 image dimensions\n",
    "hidden_size = 128\n",
    "output_size = num_classes\n",
    "\n",
    "# Initialize weights and biases with Xavier initialization\n",
    "np.random.seed(0)\n",
    "weights_input_hidden = np.random.normal(0, np.sqrt(2 / (input_size + hidden_size)), size=(input_size, hidden_size))\n",
    "bias_hidden = np.zeros(hidden_size)\n",
    "weights_hidden_output = np.random.normal(0, np.sqrt(2 / (hidden_size + output_size)), size=(hidden_size, output_size))\n",
    "bias_output = np.zeros(output_size)\n",
    "\n",
    "# Softmax activation function for the output layer\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum()\n",
    "\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(train_images)):\n",
    "        # Forward propagation\n",
    "        hidden_input = np.dot(train_images[i], weights_input_hidden) + bias_hidden\n",
    "        hidden_output = 1 / (1 + np.exp(-hidden_input))  # Sigmoid activation for the hidden layer\n",
    "        output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "        output = softmax(output_input)\n",
    "\n",
    "        # Calculate the cross-entropy loss\n",
    "        loss = -np.sum(train_labels[i] * np.log(output))\n",
    "\n",
    "        # Backpropagation\n",
    "        d_output = output - train_labels[i]\n",
    "        d_hidden = np.dot(d_output, weights_hidden_output.T) * hidden_output * (1 - hidden_output)\n",
    "\n",
    "        # Update weights and biases\n",
    "        weights_hidden_output -= np.outer(hidden_output, d_output) * learning_rate\n",
    "        bias_output -= d_output * learning_rate\n",
    "        weights_input_hidden -= np.outer(train_images[i], d_hidden) * learning_rate\n",
    "        bias_hidden -= d_hidden * learning_rate\n",
    "\n",
    "    # Calculate the test accuracy\n",
    "    correct = 0\n",
    "    for i in range(len(test_images)):\n",
    "        hidden_input = np.dot(test_images[i], weights_input_hidden) + bias_hidden\n",
    "        hidden_output = 1 / (1 + np.exp(-hidden_input))\n",
    "        output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "        output = softmax(output_input)\n",
    "        if np.argmax(output) == np.argmax(test_labels[i]):\n",
    "            correct += 1\n",
    "\n",
    "    test_accuracy = correct / len(test_images)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
